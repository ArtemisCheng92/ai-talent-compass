from typing import Dict, List, Any, Tuple
import io

import numpy as np
import pandas as pd
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.cluster import KMeans



from sklearn.cluster import KMeans  # ğŸ‘ˆ æ–°å¢

# ==== å›ºå®šç‰ˆå››ç¾¤ã€Œäººæ‰ç¾…ç›¤ã€æ¨¡å‹å¡ï¼ˆå…ˆçµ¦é è¨­æ–‡å­—ï¼Œä½ ä¹‹å¾Œå¯ä»¥æ”¹ï¼‰ ====

CLUSTER_CARDS: Dict[int, Dict[str, Any]] = {
    0: {
        "key": "growth_high_engagement",
        "name": "é«˜æŠ•å…¥æˆé•·å‹",
        "short_title": "ä¸»å‹•å­¸ç¿’ï¼Œé‚„æœ‰å¾ˆå¤§æˆé•·ç©ºé–“",
        "summary": "é€šå¸¸å­¸ç¿’å‹•æ©Ÿé«˜ã€è¨“ç·´åƒèˆ‡åº¦ä½³ï¼Œç¸¾æ•ˆå·²ä¸éŒ¯ä½†ä»åœ¨ä¸Šå‡è»Œé“ã€‚",
        "dev_focus": [
            "æä¾›å…·æŒ‘æˆ°æ€§çš„å°ˆæ¡ˆæˆ–è¼ªèª¿æ©Ÿæœƒ",
            "è¨­è¨ˆæ˜ç¢ºçš„æ™‰å‡ï¼è·æ¶¯æ™‰ç´šè·¯å¾‘",
            "å®šæœŸå›é¥‹ï¼Œå”åŠ©èª¿æ•´å­¸ç¿’é‡é»"
        ],
        "risk_alert": [
            "é¿å…é•·æœŸé«˜æŠ•å…¥å»å‡é·åœæ»¯ï¼Œå°è‡´å‹•æ©Ÿä¸‹æ»‘",
            "æ³¨æ„å·¥ä½œè² è·æ˜¯å¦éé«˜ï¼Œå½±éŸ¿å·¥ä½œç”Ÿæ´»å¹³è¡¡"
        ],
        "suggestions": [
            "å®‰æ’ä¸€ä½è³‡æ·±å°å¸«ï¼Œå”åŠ©è¦åŠƒæœªä¾† 1â€“2 å¹´çš„æˆé•·ç›®æ¨™",
            "è®“ä»–åƒèˆ‡è·¨éƒ¨é–€å°ˆæ¡ˆï¼Œæ“´å¤§å½±éŸ¿ç¯„åœ",
            "æ­é…æ˜ç¢ºçš„æŠ€èƒ½èªè­‰æˆ–å¾½ç« åˆ¶åº¦ï¼Œå¼·åŒ–æˆå°±æ„Ÿ"
        ]
    },
    1: {
        "key": "steady_veteran",
        "name": "ç©©å¥è³‡æ·±å‹",
        "short_title": "å¹´è³‡æ·±ã€è¡¨ç¾ç©©å®šçš„é—œéµæ”¯æŸ±",
        "summary": "åœ¨çµ„ç¹”å¹´è³‡è¼ƒé•·ã€ç¸¾æ•ˆç©©å®šï¼Œæ˜¯åœ˜éšŠä¸­çš„ç©©å®šåŠ›é‡èˆ‡çŸ¥è­˜ä¾†æºã€‚",
        "dev_focus": [
            "å¼·åŒ–çŸ¥è­˜å‚³æ‰¿èˆ‡æ•™ç·´è§’è‰²",
            "å”åŠ©æ›´æ–°æŠ€èƒ½ï¼Œé¿å…èˆ‡æ–°æŠ€è¡“è„«ç¯€",
            "é¼“å‹µåƒèˆ‡åˆ¶åº¦ï¼æµç¨‹å„ªåŒ–å°ˆæ¡ˆ"
        ],
        "risk_alert": [
            "ç•™æ„æ˜¯å¦å‡ºç¾å‹•èƒ½ä¸‹æ»‘æˆ–å°è®Šé©æŠ—æ‹’",
            "é¿å…åªè¢«è¦–ç‚ºã€ç©©å®šäººåŠ›ã€è€Œç¼ºä¹æˆé•·æ©Ÿæœƒ"
        ],
        "suggestions": [
            "è¨­è¨ˆã€è³‡æ·±å“¡å·¥ mentor è¨ˆç•«ã€ï¼Œç”±ä»–å¸¶æ–°åŒä»",
            "é‚€è«‹åƒèˆ‡å…§éƒ¨è¨“ç·´èª²ç¨‹æˆèª²æˆ–å…±å‚™",
            "åœ¨ç¸¾æ•ˆå°è©±ä¸­åŠ å…¥ã€å‚³æ‰¿èˆ‡å½±éŸ¿åŠ›ã€çš„æŒ‡æ¨™"
        ]
    },
    2: {
        "key": "high_pressure_risky",
        "name": "é«˜å£“é«˜é¢¨éšªå‹",
        "short_title": "è² è·é«˜ã€ç¸¾æ•ˆå¯èƒ½å…©æ¥µï¼Œéœ€è¦é¢¨éšªç®¡ç†",
        "summary": "å¸¸å‡ºç¾é«˜å·¥æ™‚ã€é«˜å£“åŠ›æˆ–é »ç¹åŠ ç­ï¼Œç¸¾æ•ˆæœ‰æ™‚äº®çœ¼ã€æœ‰æ™‚ä¸ç©©å®šã€‚",
        "dev_focus": [
            "èª¿æ•´å·¥ä½œè² è·èˆ‡è§’è‰²å®šä½ï¼Œé¿å…é•·æœŸéå‹",
            "å¼•å…¥å£“åŠ›ç®¡ç†èˆ‡å¿ƒç†è³‡æº",
            "æ˜ç¢ºè¨­å®šå„ªå…ˆé †åºèˆ‡å¯è¢«æ‹’çµ•çš„ç•Œç·š"
        ],
        "risk_alert": [
            "å€¦æ€ é¢¨éšªé«˜ï¼Œå¯èƒ½çªç„¶é›¢è·æˆ–è¡¨ç¾é©Ÿé™",
            "å®¹æ˜“å½±éŸ¿åœ˜éšŠæ°›åœï¼Œè®“å£“åŠ›æ–‡åŒ–æ“´æ•£"
        ],
        "suggestions": [
            "æª¢è¦–æ‰‹ä¸Šçš„ä»»å‹™èˆ‡ KPIï¼Œå”åŠ©åˆªæ¸›éé—œéµå·¥ä½œ",
            "æä¾›å½ˆæ€§å·¥æ™‚æˆ–ä¼‘å‡å®‰æ’ï¼Œè®“ä»–æœ‰æ¢å¾©ç©ºé–“",
            "HR å®šæœŸ 1:1 check-inï¼Œè¿½è¹¤å£“åŠ›èˆ‡å¥åº·ç‹€æ³"
        ]
    },
    3: {
        "key": "emerging_talent",
        "name": "æ–°ç§€æ½›åŠ›å‹",
        "short_title": "å¹´è³‡è¼ƒçŸ­ã€æ½›åŠ›æ˜é¡¯ï¼Œéœ€è¦è¢«å¥½å¥½æ ½åŸ¹",
        "summary": "å‰›åŠ å…¥æˆ–å¹´è³‡å°šçŸ­ï¼Œè¡¨ç¾å·²å±•ç¾æ½›åŠ›ï¼Œä½†ä»åœ¨æ‘¸ç´¢éšæ®µã€‚",
        "dev_focus": [
            "å¿«é€Ÿè£œé½Šæ ¸å¿ƒæŠ€èƒ½èˆ‡åˆ¶åº¦çŸ¥è­˜",
            "å»ºç«‹æ¸…æ¥šçš„æœŸæœ›èˆ‡å›é¥‹é »ç‡",
            "è®“ä»–æœ‰å°è¦æ¨¡è©¦éŒ¯èˆ‡å˜—è©¦çš„ç©ºé–“"
        ],
        "risk_alert": [
            "è‹¥ç¼ºä¹æŒ‡å°èˆ‡å›é¥‹ï¼Œå®¹æ˜“è¿·æƒ˜æˆ–å–ªå¤±ä¿¡å¿ƒ",
            "å¤ªå¿«å£“ä¸Šé—œéµä»»å‹™ï¼Œå¯èƒ½å£“åŠ›éå¤§"
        ],
        "suggestions": [
            "å®‰æ’å…¥è·å¾Œ 3â€“6 å€‹æœˆçš„çµæ§‹åŒ–åŸ¹è¨“èˆ‡ check-point",
            "åœ¨ç¸¾æ•ˆå°è©±ä¸­å¤šé—œæ³¨ã€å­¸ç¿’æ›²ç·šã€è€Œéå–®æ¬¡çµæœ",
            "æ­é…å¸«å¾’åˆ¶æˆ–åŒå„• buddyï¼Œæä¾›æ—¥å¸¸æ”¯æŒ"
        ]
    },
}



try:
    import shap  # type: ignore
    HAS_SHAP = True
except Exception:
    shap = None
    HAS_SHAP = False


app = FastAPI(title="AI Talent Predictor API", version="0.2")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== å…¨åŸŸç‹€æ…‹ï¼šå­˜æ¨¡å‹ã€å‰è™•ç†å™¨ã€æ¬„ä½è³‡è¨Š =====
MODEL_STATE: Dict[str, Any] = {
    "pipe_rf": None,
    "preprocessor": None,
    "numeric_cols": None,
    "categorical_cols": None,
    "feature_cols": None,
    "defaults_all": None,
    "shap_explainer": None,
    "shap_feature_names": None,
    "top_features": None,   # å·²å­˜åœ¨ï¼šçµ¦ Demo é ç”¨
    "metrics": None,        # å·²å­˜åœ¨
    "kmeans": None,         # ğŸ‘ˆ æ–°å¢ï¼šk=4 åˆ†ç¾¤æ¨¡å‹
    "perf_quantiles": None, # ğŸ‘ˆ æ–°å¢ï¼šç”¨ä¾†åˆ¤æ–·é«˜/ä¸­/ä½ç¸¾æ•ˆ band
    # ğŸ”¹ æ–°å¢ï¼šä¿ç•™å®Œæ•´ç‰¹å¾µè³‡æ–™å’Œ yï¼Œçµ¦è‡ªç”±åˆ†ç¾¤ç”¨
    "X_all": None,
    "y_all": None,
    "df_for_cluster": None,   # ğŸ‘ˆ æ–°å¢ï¼šçµ¦è‡ªç”±åˆ†ç¾¤æ²™ç›’ç”¨
}




class PredictRequest(BaseModel):
    features: Dict[str, Any]

class ClusterSandboxRequest(BaseModel):
    k: int


class TalentCompassRequest(BaseModel):
    features: Dict[str, Any]

class ClusterPlayRequest(BaseModel):
    k: int = 4   # é è¨­ 4 ç¾¤



# ===== å…±ç”¨å°å·¥å…· =====
def read_csv_upload(file: UploadFile) -> pd.DataFrame:
    content = file.file.read()
    try:
        df = pd.read_csv(io.BytesIO(content))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ç„¡æ³•è®€å– CSVï¼š{e}")
    if df.empty:
        raise HTTPException(status_code=400, detail="CSV å…§å®¹ç‚ºç©º")
    return df


def is_numeric_series(s: pd.Series) -> bool:
    return pd.api.types.is_numeric_dtype(s)


def build_preprocessor(df_features: pd.DataFrame) -> Tuple[ColumnTransformer, List[str], List[str]]:
    num_cols = [c for c in df_features.columns if is_numeric_series(df_features[c])]
    cat_cols = [c for c in df_features.columns if c not in num_cols]

    num_tf = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )
    cat_tf = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    pre = ColumnTransformer(
        [
            ("num", num_tf, num_cols),
            ("cat", cat_tf, cat_cols),
        ]
    )
    return pre, num_cols, cat_cols


# ===== æ¨¡å‹è¨“ç·´ä¸»æµç¨‹ï¼ˆç…§ä½  Step2 æ”¹æˆå‡½å¼ç‰ˆï¼‰ =====
def train_model(df: pd.DataFrame, test_size: float = 0.2) -> Dict[str, Any]:
    target_col = "Performance_Score"
    if target_col not in df.columns:
        raise HTTPException(status_code=400, detail="è³‡æ–™ä¸­æ‰¾ä¸åˆ° Performance_Score æ¬„ä½")

    # 1) é¿å…æ´©æ¼ï¼šä¸Ÿæ‰ ID / æ—¥æœŸ / ç›®æ¨™ / é›¢è·ã€æ™‰å‡ã€cluster ç­‰æ¬„ä½
    drop_cols: List[str] = []
    for col in df.columns:
        lower = col.lower()
        if "id" in lower or "date" in lower:
            drop_cols.append(col)

    drop_cols.extend(
        [
            target_col,
            "Resigned",
            "Promotions",
            "Promotion_Last_3_Years",
            "cluster_kmeans_v2",
            "cluster_kmeans",
            "y_true",
            "y_pred",
        ]
    )
    drop_cols = sorted(set([c for c in drop_cols if c in df.columns]))

    y = df[target_col].astype(float)
    feature_cols = [c for c in df.columns if c not in drop_cols]
    if not feature_cols:
        raise HTTPException(status_code=400, detail="ç§»é™¤ ID / ç›®æ¨™æ¬„ä½å¾Œæ²’æœ‰å‰©ä¸‹ä»»ä½•ç‰¹å¾µå¯ç”¨")

    X = df[feature_cols].copy()

    # çµ¦è‡ªç”±åˆ†ç¾¤æ²™ç›’ç”¨ï¼šä¿ç•™ç‰¹å¾µ + ç›®æ¨™
    MODEL_STATE["df_for_cluster"] = df[feature_cols + [target_col]].copy()

    # ğŸ”¹ æ–°å¢ï¼šæŠŠå®Œæ•´ X / y å­˜èµ·ä¾†ï¼Œè®“å¾Œé¢è‡ªç”±åˆ†ç¾¤å¯ä»¥ç”¨åŒä¸€ä»½è³‡æ–™
    MODEL_STATE["X_all"] = X.copy()
    MODEL_STATE["y_all"] = y.copy()


    # 2) å‰è™•ç† + åˆ‡è¨“ç·´ / æ¸¬è©¦
    pre, num_cols, cat_cols = build_preprocessor(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

    # 3) æ¨¡å‹ï¼šRF + ElasticNetï¼ˆåŒä½ çš„ Step2ï¼‰
    rf = RandomForestRegressor(
        n_estimators=300,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    enet = ElasticNetCV(
        l1_ratio=[0.2, 0.5, 0.8],
        alphas=None,
        cv=5,
        random_state=42,
        n_jobs=-1,
    )

    pipe_rf = Pipeline([("pre", pre), ("rf", rf)])
    pipe_en = Pipeline([("pre", pre), ("enet", enet)])

    pipe_rf.fit(X_train, y_train)
    pipe_en.fit(X_train, y_train)

    # 4) è©•ä¼°æŒ‡æ¨™
    pred_rf_train = pipe_rf.predict(X_train)
    pred_rf_test = pipe_rf.predict(X_test)
    pred_en_test = pipe_en.predict(X_test)

    metrics = {
        "rf": {
            "R2_train": float(r2_score(y_train, pred_rf_train)),
            "R2_test": float(r2_score(y_test, pred_rf_test)),
            "MAE_test": float(mean_absolute_error(y_test, pred_rf_test)),
        },
        "elastic_net": {
            "R2_test": float(r2_score(y_test, pred_en_test)),
            "MAE_test": float(mean_absolute_error(y_test, pred_en_test)),
        },
    }


    # === æ–°å¢ Aï¼šç”¨æ•´é«”è³‡æ–™ + å‰è™•ç†å™¨ è¨“ç·´ k=4 åˆ†ç¾¤ ===
    fitted_pre = pipe_rf.named_steps["pre"]
    X_all_trans = fitted_pre.transform(X)   # X æ˜¯å…¨è³‡æ–™ï¼Œä¸åªè¨“ç·´é›†
    kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)
    kmeans_4.fit(X_all_trans)

    # === æ–°å¢ Bï¼šè¨ˆç®—ç¸¾æ•ˆåˆ†å±¤ç”¨çš„åˆ†ä½æ•¸ï¼ˆä½/ä¸­/é«˜ï¼‰ ===
    q_low = float(np.quantile(y_train, 0.33))
    q_high = float(np.quantile(y_train, 0.66))
    perf_quantiles = {"low": q_low, "high": q_high}


    # 5) ç‚ºæ¯å€‹æ¬„ä½ç®—ã€Œé è¨­å€¼ã€ï¼ˆä¸­ä½æ•¸ / çœ¾æ•¸ï¼‰ï¼Œçµ¦é æ¸¬é ç”¨
    defaults_all: Dict[str, Any] = {}
    for col in feature_cols:
        s = X[col]
        if is_numeric_series(s):
            defaults_all[col] = float(s.median())
        else:
            mode = s.mode()
            defaults_all[col] = mode.iloc[0] if not mode.empty else ""

    # 6) SHAP å…¨åŸŸé‡è¦åº¦ï¼ˆèšåˆåˆ°ã€ŒåŸå§‹æ¬„ä½ã€å±¤ç´šï¼‰
    shap_explainer = None
    shap_feature_names: List[str] = []
    shap_global_agg: List[Dict[str, Any]] = []

    if HAS_SHAP:
        fitted_pre = pipe_rf.named_steps["pre"]
        fitted_rf = pipe_rf.named_steps["rf"]

        sample_size = min(1000, len(X_train))
        rng = np.random.RandomState(42)
        sample_idx = rng.choice(X_train.index, size=sample_size, replace=False)
        X_train_sample = X_train.loc[sample_idx]

        X_train_trans = fitted_pre.transform(X_train_sample)

        shap_explainer = shap.TreeExplainer(fitted_rf)
        shap_values = shap_explainer.shap_values(X_train_trans)

        # One-Hot ä¹‹å¾Œçš„ç‰¹å¾µåç¨±
        num_features = list(num_cols)
        cat_feature_names: List[str] = []
        if cat_cols:
            ohe = fitted_pre.named_transformers_["cat"].named_steps["onehot"]
            cat_feature_names = list(ohe.get_feature_names_out(cat_cols))
        feature_names = num_features + cat_feature_names
        shap_feature_names = feature_names

        mean_abs_shap = np.abs(shap_values).mean(axis=0)

        # å°‡ Job_Title_Technician / Job_Title_Analyst ... èšåˆå› Job_Title
        agg: Dict[str, float] = {}
        for fname, val in zip(feature_names, mean_abs_shap):
            if fname in num_cols:
                base = fname
            else:
                base = None
                for cat in cat_cols:
                    prefix = f"{cat}_"
                    if fname.startswith(prefix):
                        base = cat
                        break
                if base is None:
                    base = fname
            agg[base] = agg.get(base, 0.0) + float(val)

        shap_global_agg = [
            {"feature": base, "mean_abs_shap": float(val)}
            for base, val in sorted(agg.items(), key=lambda kv: kv[1], reverse=True)
        ]
        shap_global_agg = shap_global_agg[:15]

    # 7) çµ„åˆå‰ç«¯æœƒç”¨åˆ°çš„æ¬„ä½è³‡è¨Šï¼ˆå«æç¤ºï¼‰
    top_features_with_defaults: List[Dict[str, Any]] = []

    for row in shap_global_agg:
        fname = row["feature"]
        row_out: Dict[str, Any] = dict(row)
        default_val = defaults_all.get(fname)
        row_out["default"] = default_val

        if fname in num_cols:
            row_out["value_type"] = "number"
            # çµ¦ä¸€å€‹å¤§æ¦‚ç¯„åœçš„æç¤º
            hint = None
            try:
                s = df[fname]
                p5 = float(np.nanpercentile(s, 5))
                p95 = float(np.nanpercentile(s, 95))
                med = float(np.nanmedian(s))
                hint = f"å»ºè­°è¼¸å…¥æ•¸å­—ï¼Œå¸¸è¦‹ç¯„åœç´„ {p5:.1f}â€“{p95:.1f}ï¼ˆä¸­ä½æ•¸ {med:.1f}ï¼‰"
            except Exception:
                hint = "è«‹è¼¸å…¥æ•¸å­—ï¼ˆå–®ä½åŒåŸå§‹è³‡æ–™ï¼‰ã€‚"
            row_out["hint"] = hint
        else:
            row_out["value_type"] = "text"
            hint = None
            try:
                if fname in df.columns:
                    s = df[fname].astype(str)
                    top_vals = s.value_counts().head(4).index.tolist()
                    if top_vals:
                        joined = " / ".join(map(str, top_vals))
                        hint = f"å»ºè­°è¼¸å…¥è³‡æ–™ä¸­å‡ºç¾éçš„æ–‡å­—ï¼Œä¾‹å¦‚ï¼š{joined}"
            except Exception:
                pass
            if not hint:
                hint = "è«‹è¼¸å…¥æ–‡å­—ï¼ˆç›¡é‡ä½¿ç”¨è³‡æ–™é›†ä¸­å‡ºç¾éçš„é¡åˆ¥ï¼‰ã€‚"
            row_out["hint"] = hint

        top_features_with_defaults.append(row_out)

    # 8) æŠŠæ¨¡å‹ç›¸é—œç‰©ä»¶å­˜åˆ°å…¨åŸŸç‹€æ…‹ï¼Œæ–¹ä¾¿ /api/predict èˆ‡ /api/model_summary ä½¿ç”¨
    MODEL_STATE["pipe_rf"] = pipe_rf
    MODEL_STATE["preprocessor"] = pipe_rf.named_steps["pre"]
    MODEL_STATE["numeric_cols"] = num_cols
    MODEL_STATE["categorical_cols"] = cat_cols
    MODEL_STATE["feature_cols"] = feature_cols
    MODEL_STATE["defaults_all"] = defaults_all
    MODEL_STATE["shap_explainer"] = shap_explainer
    MODEL_STATE["shap_feature_names"] = shap_feature_names
    MODEL_STATE["top_features"] = top_features_with_defaults  # âœ… ç¾åœ¨è®Šæ•¸å·²ç¶“å…ˆç®—å¥½
    MODEL_STATE["metrics"] = metrics

    # ğŸ‘‡ æ–°å¢ï¼šå­˜ k=4 èˆ‡åˆ†ä½æ•¸
    MODEL_STATE["kmeans"] = kmeans_4
    MODEL_STATE["perf_quantiles"] = perf_quantiles

    return {
        "target_col": target_col,
        "n_rows": int(len(df)),
        "n_features": int(len(feature_cols)),
        "metrics": metrics,
        "top_features": top_features_with_defaults,
        "has_shap": HAS_SHAP,
    }



# ===== APIï¼šä¸Šå‚³ CSV â†’ è¨“ç·´æ¨¡å‹ =====
@app.post("/api/train_model")
async def api_train_model(
    file: UploadFile = File(...),
    test_size: float = Form(0.2),
):
    df = read_csv_upload(file)
    result = train_model(df, test_size=test_size)
    return result


# ===== APIï¼šå–®ä¸€æ¨£æœ¬é æ¸¬ =====
@app.post("/api/predict")
async def api_predict(req: PredictRequest = Body(...)):
    if MODEL_STATE["pipe_rf"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨é æ¸¬é é¢ä¸Šå‚³è³‡æ–™ä¸¦åŸ·è¡Œã€Œå»ºç«‹æ¨¡å‹ã€",
        )

    pipe_rf: Pipeline = MODEL_STATE["pipe_rf"]
    pre = MODEL_STATE["preprocessor"]
    feature_cols: List[str] = MODEL_STATE["feature_cols"]
    defaults_all: Dict[str, Any] = MODEL_STATE["defaults_all"] or {}
    shap_explainer = MODEL_STATE["shap_explainer"]
    shap_feature_names: List[str] = MODEL_STATE["shap_feature_names"] or []

    # ä»¥è¨“ç·´è³‡æ–™çš„ã€Œé è¨­å€¼ã€ç•¶æˆ base å€‹æ¡ˆï¼Œå†ç”¨ä½¿ç”¨è€…è¼¸å…¥è¦†è“‹
    row_data: Dict[str, Any] = {}
    for col in feature_cols:
        row_data[col] = defaults_all.get(col)

    for k, v in req.features.items():
        if k in row_data:
            row_data[k] = v

    X_new = pd.DataFrame([row_data], columns=feature_cols)

    y_pred = float(pipe_rf.predict(X_new)[0])

    # SHAP å€‹æ¡ˆè²¢ç»ï¼ˆTop10ï¼‰
    shap_details: List[Dict[str, Any]] = []
    if HAS_SHAP and shap_explainer is not None and shap_feature_names:
        X_new_trans = pre.transform(X_new)
        shap_values = shap_explainer.shap_values(X_new_trans)[0]
        abs_vals = np.abs(shap_values)
        order = np.argsort(-abs_vals)
        top_k = min(10, len(order))
        for idx in order[:top_k]:
            shap_details.append(
                {
                    "feature": shap_feature_names[idx],
                    "shap_value": float(shap_values[idx]),
                    "abs_shap": float(abs_vals[idx]),
                }
            )

    return {
        "prediction": y_pred,
        "shap_top_contrib": shap_details,
    }


from fastapi import Body

# ï¼ˆå‰é¢ CLUSTER_CARDSã€BAND_THRESHOLDS ç­‰ä¿æŒä¸å‹•ï¼‰

@app.post("/api/talent_compass_predict")
async def api_talent_compass_predict(req: PredictRequest = Body(...)):
    if MODEL_STATE["pipe_rf"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨é æ¸¬é é¢ä¸Šå‚³è³‡æ–™ä¸¦åŸ·è¡Œã€Œå»ºç«‹æ¨¡å‹ã€",
        )

    pipe_rf: Pipeline = MODEL_STATE["pipe_rf"]
    pre = MODEL_STATE["preprocessor"]
    feature_cols: List[str] = MODEL_STATE["feature_cols"] or []
    defaults_all: Dict[str, Any] = MODEL_STATE["defaults_all"] or {}

    # 1) çµ„ä¸€åˆ—è³‡æ–™ï¼ˆç”¨ defaults ç•¶åº•ï¼Œå†ç”¨ä½¿ç”¨è€…è¼¸å…¥è¦†è“‹ï¼‰
    row_data = {col: defaults_all.get(col) for col in feature_cols}
    for k, v in req.features.items():
        if k in row_data:
            row_data[k] = v

    X_new = pd.DataFrame([row_data], columns=feature_cols)

    # 2) å…ˆç”¨ RF é æ¸¬ç¸¾æ•ˆåˆ†æ•¸
    score = float(pipe_rf.predict(X_new)[0])

    # 3) ç”¨è¨“ç·´æ™‚ç®—å¥½çš„åˆ†ä½æ•¸ï¼Œæ±ºå®š high / medium / low
    perf_q = MODEL_STATE.get("perf_quantiles") or {}
    q_low = perf_q.get("low")
    q_high = perf_q.get("high")

    if q_low is not None and q_high is not None:
        if score >= q_high:
            band_key = "high"
        elif score >= q_low:
            band_key = "medium"
        else:
            band_key = "low"
    else:
        # å¦‚æœæ²’ç®—åˆ°åˆ†ä½æ•¸ï¼Œå°±ç”¨å›ºå®šé–€æª»ç•¶å‚™æ´
        if score >= 4.2:
            band_key = "high"
        elif score >= 3.4:
            band_key = "medium"
        else:
            band_key = "low"

    BAND_LABEL_ZH = {
        "high": "é«˜ç¸¾æ•ˆå¸¶",
        "medium": "ä¸­ç­‰ç¸¾æ•ˆå¸¶",
        "low": "éœ€é—œæ³¨ç¸¾æ•ˆå¸¶",
    }

    # 4) ç”¨ k-means åˆ†ç¾¤ï¼ˆçœŸæ­£å°æ‡‰åˆ°ä½ ç ”ç©¶çš„ 4 ç¾¤ï¼‰
    kmeans: KMeans | None = MODEL_STATE.get("kmeans")
    if kmeans is not None:
        X_new_trans = pre.transform(X_new)
        cluster_id = int(kmeans.predict(X_new_trans)[0])  # 0~3
    else:
        cluster_id = 0  # å‚™æ´

    # 5) ç”¨æ•´æ•¸ cluster_id å»æ‹¿å°æ‡‰çš„äººæ‰å¡
    card = CLUSTER_CARDS.get(cluster_id, {})

    dev_focus = card.get("dev_focus", [])
    # æ³¨æ„ï¼šåŸæœ¬ key å« risk_alert / suggestionsï¼Œé€™é‚Šå¹«ä½ è½‰ä¸€ä¸‹
    risk_alerts = card.get("risk_alert", [])
    hr_tips = card.get("suggestions", [])

    payload = {
        "performance_score": score,
        "performance_band": band_key,
        "performance_level": BAND_LABEL_ZH.get(band_key, "å°šæœªæ¨™å®šç­‰ç´š"),

        "cluster_id": cluster_id,
        "cluster_name": card.get("name", f"ç¬¬ {cluster_id + 1} ç¾¤"),
        "cluster_short_title": card.get("short_title", ""),
        "cluster_summary": card.get("summary", ""),

        "dev_focus": dev_focus,
        "risk_alerts": risk_alerts,
        "hr_tips": hr_tips,

        "cluster_card": card,
    }
    return payload



@app.post("/api/cluster_sandbox")
async def api_cluster_sandbox(req: ClusterSandboxRequest = Body(...)):
    """
    è‡ªç”±åˆ†ç¾¤éŠæ¨‚å ´ç”¨ï¼š
    - ä½¿ç”¨ Demo å·²è¨“ç·´å¥½çš„è³‡æ–™èˆ‡å‰è™•ç†
    - åªèª¿æ•´ kï¼ˆç¾¤æ•¸ï¼‰ï¼Œå›å‚³å„ç¾¤äººæ•¸ã€å¹³å‡ç¸¾æ•ˆèˆ‡ã€Œç›¸å°æ•´é«”çš„ç‰¹å¾µè¼ªå»“ã€
    """
    if MODEL_STATE["df_for_cluster"] is None or MODEL_STATE["preprocessor"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹æˆ–å°šæœªè¼‰å…¥åŸå§‹è³‡æ–™ï¼Œè«‹å…ˆåœ¨ Demo é ä¸Šå‚³è³‡æ–™ä¸¦å»ºç«‹æ¨¡å‹ã€‚",
        )

    k = max(2, int(req.k))
    df_cluster: pd.DataFrame = MODEL_STATE["df_for_cluster"].copy()
    feature_cols: List[str] = MODEL_STATE["feature_cols"] or []
    pre = MODEL_STATE["preprocessor"]

    if not feature_cols:
        raise HTTPException(status_code=400, detail="æ‰¾ä¸åˆ°å¯ç”¨ç‰¹å¾µæ¬„ä½ã€‚")

    X_all = df_cluster[feature_cols]
    y_all = df_cluster["Performance_Score"].astype(float)
    n_samples = len(df_cluster)

    # å…ˆç”¨æ—¢æœ‰å‰è™•ç†è½‰æ›ï¼Œå†åš k-means åˆ†ç¾¤
    X_all_trans = pre.transform(X_all)

    km = KMeans(
        n_clusters=k,
        random_state=42,
        n_init="auto",
    )
    labels = km.fit_predict(X_all_trans)
    df_cluster["cluster"] = labels

    overall_mean = float(y_all.mean())

    # ğŸ”¹ åªé‡å°ã€Œæ•¸å€¼ç‰¹å¾µã€åšè¼ªå»“æ¯”è¼ƒ
    numeric_cols: List[str] = MODEL_STATE["numeric_cols"] or []
    numeric_cols = [c for c in numeric_cols if c in df_cluster.columns]

    if numeric_cols:
        global_feature_means = (
            df_cluster[numeric_cols].mean(numeric_only=True).to_dict()
        )
    else:
        global_feature_means = {}

    clusters_out: List[Dict[str, Any]] = []

    # ä¾å„ç¾¤å¹³å‡ç¸¾æ•ˆæ’åºå¾Œå†ç”¢å‡ºèªªæ˜
    cluster_stats: List[Tuple[int, float]] = []
    for cid in range(k):
        mask = df_cluster["cluster"] == cid
        if mask.sum() == 0:
            continue
        mean_perf = float(df_cluster.loc[mask, "Performance_Score"].mean())
        cluster_stats.append((cid, mean_perf))

    # ä¾å¹³å‡ç¸¾æ•ˆç”±é«˜åˆ°ä½æ’åº
    cluster_stats.sort(key=lambda x: x[1], reverse=True)

    for rank_idx, (cid, mean_perf) in enumerate(cluster_stats):
        mask = df_cluster["cluster"] == cid
        sub = df_cluster.loc[mask]
        n_c = int(mask.sum())
        prop = n_c / n_samples if n_samples > 0 else 0.0
        std_perf = float(sub["Performance_Score"].std(ddof=0) or 0.0)

        # å¹³å‡åˆ†æ•¸åœ¨æ•´é«”çš„ç²—ç•¥ç™¾åˆ†ä½
        mean_percentile = float((y_all < mean_perf).mean()) if n_samples > 0 else 0.0

        if rank_idx == 0:
            rank_label = "æ•´é«”é«˜ç¸¾æ•ˆè¼ªå»“"
            comment = "é€™ä¸€ç¾¤çš„å¹³å‡ç¸¾æ•ˆæœ€é«˜ï¼Œé©åˆæ·±å…¥è§€å¯Ÿå…¶å…±åŒç‰¹å¾µï¼Œä½œç‚ºé—œéµäººæ‰è¼ªå»“çš„åƒè€ƒã€‚"
        elif rank_idx == len(cluster_stats) - 1:
            rank_label = "ç›¸å°ä½ç¸¾æ•ˆè¼ªå»“"
            comment = "å¹³å‡ç¸¾æ•ˆæ˜é¡¯ä½æ–¼å…¶ä»–ç¾¤ï¼Œé©åˆæ­é…è¨“ç·´èˆ‡å·¥ä½œè¨­è¨ˆï¼Œæ€è€ƒå¦‚ä½•æ‹‰æŠ¬è¡¨ç¾ã€‚"
        else:
            rank_label = "ä¸­é–“ç¸¾æ•ˆè¼ªå»“"
            comment = "å¹³å‡ç¸¾æ•ˆä»‹æ–¼å…©ç«¯ä¹‹é–“ï¼Œå¯å†ä¾å·¥ä½œå…§å®¹ã€å¹´è³‡ç­‰è®Šé …æ‹†è§£å­è¼ªå»“ã€‚"

        # ğŸ”¹ æœ¬ç¾¤å„æ•¸å€¼ç‰¹å¾µå¹³å‡ï¼Œèˆ‡æ•´é«”æ¯”è¼ƒ
        if numeric_cols:
            feature_means = (
                sub[numeric_cols].mean(numeric_only=True).to_dict()
            )
        else:
            feature_means = {}

        diff_list: List[Dict[str, Any]] = []
        for col in numeric_cols:
            g = global_feature_means.get(col)
            c_mean = feature_means.get(col)
            if g is None or c_mean is None or pd.isna(g) or pd.isna(c_mean):
                continue
            diff_val = float(c_mean - g)
            diff_list.append(
                {
                    "feature": col,
                    "cluster_mean": float(c_mean),
                    "global_mean": float(g),
                    "diff": diff_val,
                }
            )

        diff_list.sort(key=lambda d: abs(d["diff"]), reverse=True)
        top_feature_diff = diff_list[:5]

        clusters_out.append(
            {
                "cluster_id": cid,
                "display_name": f"ç¬¬ {cid + 1} ç¾¤",
                "n_samples": n_c,
                "proportion": float(prop),
                "mean_performance": mean_perf,
                "std_performance": std_perf,
                "mean_percentile": mean_percentile,
                "rank_label": rank_label,
                "comment": comment,
                "feature_means": feature_means,      # â­ æ–°å¢ï¼šæ¯ç¾¤å…¨éƒ¨æ•¸å€¼ç‰¹å¾µå¹³å‡
                "top_feature_diff": top_feature_diff,  # â­ çµ¦å‰ç«¯ç”¨
            }
        )

    return {
        "k": k,
        "n_samples": n_samples,
        "overall_mean_performance": overall_mean,
        "feature_means_global": global_feature_means,  # â­ å…¨é«” baseline
        "clusters": clusters_out,
    }



    
@app.post("/api/cluster_playground")
async def api_cluster_playground(req: ClusterPlayRequest = Body(...)):
    """
    è‡ªç”±ç‰ˆåˆ†ç¾¤éŠæ¨‚å ´ï¼š
    - ä½¿ç”¨ç›®å‰è¨“ç·´å¥½çš„è³‡æ–™ï¼ˆX_all / y_allï¼‰
    - åªç”¨æ•¸å€¼æ¬„ä½åš KMeans åˆ†ç¾¤
    - å›å‚³æ¯ä¸€ç¾¤çš„äººæ•¸ã€æ¯”ä¾‹ã€å¹³å‡ Performance_Scoreã€ä»¥åŠå·®ç•°æœ€å¤§çš„ä¸€äº›æ•¸å€¼ç‰¹å¾µ
    """
    if MODEL_STATE["X_all"] is None or MODEL_STATE["y_all"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨ç ”ç©¶ç‰ˆé é¢ä¸Šå‚³è³‡æ–™ä¸¦å»ºç«‹æ¨¡å‹ã€‚",
        )

    X_all: pd.DataFrame = MODEL_STATE["X_all"]
    y_all: pd.Series = MODEL_STATE["y_all"]

    k = int(req.k)
    if k < 2 or k > 8:
        raise HTTPException(
            status_code=400,
            detail="ç¾¤æ•¸ k å»ºè­°ä»‹æ–¼ 2ï½8ï¼ˆå¤ªå¤šç¾¤å¯è®€æ€§æœƒè®Šå·®ï¼‰ã€‚",
        )

    num_cols: List[str] = MODEL_STATE["numeric_cols"] or []
    if not num_cols:
        raise HTTPException(status_code=400, detail="æ‰¾ä¸åˆ°å¯ç”¨çš„æ•¸å€¼æ¬„ä½ï¼Œç„¡æ³•åˆ†ç¾¤ã€‚")

    # åªç”¨æ•¸å€¼æ¬„ä½åšåˆ†ç¾¤ï¼Œé¿å…æ–‡å­—æ¬„ä½çš„è™•ç†å•é¡Œ
    X_num = X_all[num_cols].copy()

    # ç°¡å–®ç‰ˆå‰è™•ç†ï¼šç¼ºå€¼è£œä¸­ä½æ•¸ï¼‹æ¨™æº–åŒ– â†’ KMeans
    cluster_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
            ("kmeans", KMeans(n_clusters=k, random_state=42, n_init="auto")),
        ]
    )
    cluster_pipe.fit(X_num)

    labels = cluster_pipe.named_steps["kmeans"].labels_
    n_total = len(X_num)
    overall_means = X_num.mean(axis=0)
    perf_overall = float(y_all.mean())

    clusters_out: List[Dict[str, Any]] = []
    for cid in range(k):
        mask = labels == cid
        n_cluster = int(mask.sum())
        if n_cluster == 0:
            continue

        frac = float(n_cluster / n_total)
        cluster_means = X_num[mask].mean(axis=0)
        perf_mean = float(y_all[mask].mean())

        # æ‰¾å‡ºã€Œæ­¤ç¾¤ vs å…¨é«”ã€å·®ç•°æœ€å¤§çš„å‰ 5 å€‹ç‰¹å¾µ
        diff = (cluster_means - overall_means).abs().sort_values(ascending=False)
        top_features: List[Dict[str, Any]] = []
        for fname in diff.index[:5]:
            top_features.append(
                {
                    "feature": fname,
                    "cluster_mean": float(cluster_means[fname]),
                    "overall_mean": float(overall_means[fname]),
                }
            )

        clusters_out.append(
            {
                "cluster_id": cid,
                "size": n_cluster,
                "ratio": frac,
                "performance_mean": perf_mean,
                "performance_overall": perf_overall,
                "top_diff_features": top_features,
            }
        )

    return {
        "k": k,
        "n_rows": n_total,
        "numeric_features": num_cols,
        "clusters": clusters_out,
    }




@app.get("/api/model_summary")
async def api_model_summary():
    """
    å›å‚³ç›®å‰å·²è¨“ç·´æ¨¡å‹çš„æ‘˜è¦è³‡è¨Šï¼š
    - metricsï¼šRF / ElasticNet çš„ R2 / MAE
    - top_featuresï¼šå‰å¹¾å€‹ SHAP é‡è¦ç‰¹å¾µï¼ˆå«é è¨­å€¼èˆ‡æç¤ºï¼‰
    """
    if MODEL_STATE["pipe_rf"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåˆ°ã€Œé æ¸¬ Demo é ã€ä¸Šå‚³è³‡æ–™ä¸¦å»ºç«‹æ¨¡å‹ã€‚",
        )

    return {
        "has_model": True,
        "metrics": MODEL_STATE.get("metrics") or {},
        "top_features": MODEL_STATE.get("top_features") or [],
        "feature_cols": MODEL_STATE.get("feature_cols") or [],
    }
